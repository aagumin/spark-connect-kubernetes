replicaCount: 1

image:
  repository: registry.gitlab.amazmetest.ru/ml/sdk/base-images/spark-connect
  tag: 3.5.3-celeborn-aws334-mtls
  pullPolicy: Always
  imagePullSecrets: 
    - name: registry-credentials

nameOverride: ""
fullnameOverride: ""

spark:
  dynamicAllocation:
    enabled: true


  eventLog:
    enabled: true
    dir: "s3a://sdk-test/spark-connect/events/"

  celeborn:
    enabled: true
    masterEndpoints: "celeborn-master-0.celeborn-master-svc.st-aagumin-test1-amazme-38969:9097"

  awsEndpoint: "https://obs.ru-moscow-1.hc.sbercloud.ru"
  driver:
    cores: 8
    memoryMiB: 20480
    memoryOverheadMiB: 1024
    ephemeralLocalVolume: {}
    affinity:
        nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                    -   matchExpressions:
                            -   key: "app.kubernetes.io/component"
                                operator: "In"
                                values:
                                    - "spark"
    tolerations:
      - key: "app"
        operator: "Equal"
        value: "spark"
        effect: "NoExecute"

  executor:
    cores: 8
    requestCoresMilliCPU: 4000
    memoryMiB: 20480
    memoryOverheadMiB: 5120
    ephemeralLocalVolume: {}
    minExecutors: 1
    maxExecutors: 10
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
                -   matchExpressions:
                        -   key: "app.kubernetes.io/component"
                            operator: "In"
                            values:
                                - "spark"

    tolerations:
      - key: "app"
        operator: "Equal"
        value: "spark"
        effect: "NoExecute"
  scratchDir: /tmp
  kubernetesEndpoint: "https://kubernetes.default.svc.cluster.local:443"
  packages: []
  sparkConfig:
    spark.hadoop.fs.s3a.endpoint: "https://obs.ru-moscow-1.hc.sbercloud.ru"
    spark.hadoop.fs.s3a.experimental.input.fadvise: random
    spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.aws.credentials.provider: com.amazonaws.auth.EnvironmentVariableCredentialsProvider
    spark.hadoop.fs.s3a.path.style.access: false
    spark.hadoop.fs.s3a.connection.ssl.enabled: false
    spark.sql.execution.arrow.pyspark.enabled: true
    spark.sql.execution.arrow.pyspark.fallback.enabled: true
    spark.sql.files.maxPartitionBytes: 128MB
    spark.sql.parquet.datetimeRebaseModeInWrite: CORRECTED
    spark.sql.catalogImplementation: hive


    spark.serializer: org.apache.spark.serializer.KryoSerializer



    # Support ShuffleManager when defined in user jars
    # Required Spark version < 4.0.0 or without SPARK-45762, highly recommended to false for ShuffleManager in user-defined jar specified by --jars or spark.jars


  hiveMetastoreConfigMap: spark-connect-hive

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "spark-connect"
  automountServiceAccountToken: true

service:
  # Specifies whether a service should be created
  create: true
  # Annotations to add to the service
  annotations: {}
  # The name of the service to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "spark-connect"

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext:
  runAsUser: 185
  runAsGroup: 185

command: []
extraArgs: []
extraEnv:
  - name: "AWS_ACCESS_KEY_ID"
    value: "NYL9ZU7FC4AUJIAXAMTD"
  - name: "AWS_SECRET_ACCESS_KEY"
    value: "e2JCz8GpkKPxdmzpEAsdkjVvtzgcnk2JsYcPj6Wv"

containerPorts:
  sparkUi: 4040
  sparkConnect: 15002

mtls:
  enabled: false
  prestartscript: "/opt/scripts/wait_for_istio_sidecar.sh"
  poststopscript: "/opt/scripts/stop_istio_sidecar.sh"
