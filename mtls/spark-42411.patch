From e1f51475d1e031f58d32bb6075f021c1260f10f4 Mon Sep 17 00:00:00 2001
From: Puneet Gupta <puneet.gupta2@workday.com>
Date: Mon, 3 Apr 2023 15:15:58 -0700
Subject: [PATCH] Support for Istio with Strict MTLS

---
 .../spark/internal/config/package.scala       |   5 +
 .../org/apache/spark/deploy/k8s/Config.scala  |  31 ++++
 .../apache/spark/deploy/k8s/Constants.scala   |   4 +
 .../spark/deploy/k8s/KubernetesConf.scala     |   5 +
 .../spark/deploy/k8s/KubernetesUtils.scala    |  48 +++++-
 .../k8s/features/BasicDriverFeatureStep.scala |  16 +-
 .../features/BasicExecutorFeatureStep.scala   |   9 +-
 .../features/DriverServiceFeatureStep.scala   |   5 +-
 .../features/ExecutorServiceFeatureStep.scala |  86 ++++++++++
 .../cluster/k8s/ExecutorPodsAllocator.scala   |  20 ++-
 .../k8s/KubernetesExecutorBuilder.scala       |  10 +-
 .../spark/deploy/k8s/KubernetesTestConf.scala |   7 +-
 .../BasicDriverFeatureStepSuite.scala         |  28 ++++
 .../BasicExecutorFeatureStepSuite.scala       |  30 ++++
 .../DriverServiceFeatureStepSuite.scala       |   6 +-
 .../ExecutorServiceFeatureStepSuite.scala     | 147 ++++++++++++++++++
 .../k8s/KubernetesExecutorBuilderSuite.scala  |  23 +++
 .../src/main/dockerfiles/spark/entrypoint.sh  |  13 +-
 18 files changed, 465 insertions(+), 28 deletions(-)
 create mode 100644 resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/ExecutorServiceFeatureStep.scala
 create mode 100644 resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/ExecutorServiceFeatureStepSuite.scala

diff --git a/core/src/main/scala/org/apache/spark/internal/config/package.scala b/core/src/main/scala/org/apache/spark/internal/config/package.scala
index 7f93bf7621644..ced677df3fb92 100644
--- a/core/src/main/scala/org/apache/spark/internal/config/package.scala
+++ b/core/src/main/scala/org/apache/spark/internal/config/package.scala
@@ -1061,6 +1061,11 @@ package object config {
     .version("2.1.0")
     .fallbackConf(DRIVER_HOST_ADDRESS)

+  private[spark] val EXECUTOR_BIND_ADDRESS = ConfigBuilder("spark.executor.bindAddress")
+    .doc("Address where to bind network listen sockets on the executor.")
+    .stringConf
+    .createOptional
+
   private[spark] val BLOCK_MANAGER_PORT = ConfigBuilder("spark.blockManager.port")
     .doc("Port to use for the block manager when a more specific setting is not provided.")
     .version("1.1.0")
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala
index 042e96827304a..04451594fb171 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala
@@ -416,6 +416,35 @@ private[spark] object Config extends Logging {
       .stringConf
       .createOptional

+  val KUBERNETES_PRE_START_SCRIPT =
+    ConfigBuilder("spark.kubernetes.pre.start.script")
+      .internal()
+      .doc("Path to a script which would be run before driver or " +
+        "executor process is started in the containers. It is useful in cases" +
+        "where driver/executor pods have sidecars that need to be up before driver" +
+        "or executor jvm can be started")
+      .version("3.1.1")
+      .stringConf
+      .createOptional
+
+  val KUBERNETES_POST_STOP_SCRIPT =
+    ConfigBuilder("spark.kubernetes.post.stop.script")
+      .internal()
+      .doc("Path to a script which would be run after driver or " +
+        "executor process is completed. It is useful in cases" +
+        "where driver/executor pods have sidecars that need to be " +
+        "terminated once driver/executor jvm is completed")
+      .version("3.1.1")
+      .stringConf
+      .createOptional
+
+  val KUBERNETES_EXECUTORS_SVC =
+    ConfigBuilder("spark.kubernetes.executor.service")
+      .doc("If set to true then each executor pod will have an associated service entry")
+      .version("3.1.1")
+      .booleanConf
+      .createWithDefault(false)
+
   val KUBERNETES_EXECUTOR_DECOMMISSION_LABEL_VALUE =
     ConfigBuilder("spark.kubernetes.executor.decommissionLabelValue")
       .doc("Label value to apply to a pod which is being decommissioned." +
@@ -754,6 +783,8 @@ private[spark] object Config extends Logging {
   val KUBERNETES_DRIVER_ANNOTATION_PREFIX = "spark.kubernetes.driver.annotation."
   val KUBERNETES_DRIVER_SERVICE_LABEL_PREFIX = "spark.kubernetes.driver.service.label."
   val KUBERNETES_DRIVER_SERVICE_ANNOTATION_PREFIX = "spark.kubernetes.driver.service.annotation."
+  val KUBERNETES_EXECUTOR_SERVICE_ANNOTATION_PREFIX =
+    "spark.kubernetes.executor.service.annotation."
   val KUBERNETES_DRIVER_SECRETS_PREFIX = "spark.kubernetes.driver.secrets."
   val KUBERNETES_DRIVER_SECRET_KEY_REF_PREFIX = "spark.kubernetes.driver.secretKeyRef."
   val KUBERNETES_DRIVER_VOLUMES_PREFIX = "spark.kubernetes.driver.volumes."
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala
index 385734c557a38..4ccc1ebf4ee9b 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala
@@ -49,6 +49,7 @@ private[spark] object Constants {
   // Default and fixed ports
   val DEFAULT_DRIVER_PORT = 7078
   val DEFAULT_BLOCKMANAGER_PORT = 7079
+  val ALL_IPS = "0.0.0.0"
   val DRIVER_PORT_NAME = "driver-rpc-port"
   val BLOCK_MANAGER_PORT_NAME = "blockmanager"
   val UI_PORT_NAME = "spark-ui"
@@ -60,12 +61,15 @@ private[spark] object Constants {
   val ENV_EXECUTOR_MEMORY = "SPARK_EXECUTOR_MEMORY"
   val ENV_EXECUTOR_DIRS = "SPARK_EXECUTOR_DIRS"
   val ENV_APPLICATION_ID = "SPARK_APPLICATION_ID"
+  val ENV_PRE_START_SCRIPT = "SPARK_PRE_START_SCRIPT"
+  val ENV_POST_STOP_SCRIPT = "SPARK_POST_STOP_SCRIPT"
   val ENV_EXECUTOR_ID = "SPARK_EXECUTOR_ID"
   val ENV_EXECUTOR_POD_IP = "SPARK_EXECUTOR_POD_IP"
   val ENV_EXECUTOR_POD_NAME = "SPARK_EXECUTOR_POD_NAME"
   val ENV_JAVA_OPT_PREFIX = "SPARK_JAVA_OPT_"
   val ENV_CLASSPATH = "SPARK_CLASSPATH"
   val ENV_DRIVER_BIND_ADDRESS = "SPARK_DRIVER_BIND_ADDRESS"
+  val ENV_EXECUTOR_BIND_ADDRESS = "SPARK_EXECUTOR_BIND_ADDRESS"
   val ENV_SPARK_CONF_DIR = "SPARK_CONF_DIR"
   val ENV_SPARK_USER = "SPARK_USER"
   val ENV_RESOURCE_PROFILE_ID = "SPARK_RESOURCE_PROFILE_ID"
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala
index d8cb881bf0826..9375f3b697bf0 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala
@@ -190,6 +190,11 @@ private[spark] class KubernetesExecutorConf(
     KubernetesUtils.parsePrefixedKeyValuePairs(sparkConf, KUBERNETES_EXECUTOR_ANNOTATION_PREFIX)
   }

+  def serviceAnnotations: Map[String, String] = {
+    KubernetesUtils.parsePrefixedKeyValuePairs(sparkConf,
+      KUBERNETES_EXECUTOR_SERVICE_ANNOTATION_PREFIX)
+  }
+
   override def secretNamesToMountPaths: Map[String, String] = {
     KubernetesUtils.parsePrefixedKeyValuePairs(sparkConf, KUBERNETES_EXECUTOR_SECRETS_PREFIX)
   }
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala
index 0b9b1f85fb432..a22c07cd833d6 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala
@@ -23,7 +23,7 @@ import java.util.{Collections, UUID}

 import scala.collection.JavaConverters._

-import io.fabric8.kubernetes.api.model.{Container, ContainerBuilder, ContainerStateRunning, ContainerStateTerminated, ContainerStateWaiting, ContainerStatus, EnvVar, EnvVarBuilder, EnvVarSourceBuilder, HasMetadata, OwnerReferenceBuilder, Pod, PodBuilder, Quantity}
+import io.fabric8.kubernetes.api.model._
 import io.fabric8.kubernetes.client.KubernetesClient
 import org.apache.commons.codec.binary.Hex
 import org.apache.hadoop.fs.{FileSystem, Path}
@@ -31,7 +31,8 @@ import org.apache.hadoop.fs.{FileSystem, Path}
 import org.apache.spark.{SparkConf, SparkException}
 import org.apache.spark.annotation.{DeveloperApi, Since, Unstable}
 import org.apache.spark.deploy.SparkHadoopUtil
-import org.apache.spark.deploy.k8s.Config.KUBERNETES_FILE_UPLOAD_PATH
+import org.apache.spark.deploy.k8s.Config.{KUBERNETES_FILE_UPLOAD_PATH, KUBERNETES_POST_STOP_SCRIPT, KUBERNETES_PRE_START_SCRIPT}
+import org.apache.spark.deploy.k8s.Constants._
 import org.apache.spark.internal.Logging
 import org.apache.spark.launcher.SparkLauncher
 import org.apache.spark.resource.ResourceUtils
@@ -66,6 +67,49 @@ object KubernetesUtils extends Logging {
     sparkConf.getAllWithPrefix(prefix).toMap
   }

+
+  def verifyBindAddress (conf: KubernetesConf, key: String): Unit = {
+    conf.getOption(key) match {
+      case Some(x) => require(x.isEmpty || x.equals(ALL_IPS),
+        s"$key is not supported in Kubernetes mode," +
+          s" as the bind address can either be $ALL_IPS" +
+          s" or the pod's IP address.")
+      case _ => None
+    }
+  }
+
+  def getContainerWithBindAddressEnv(conf: KubernetesConf, confKey: String,
+                                     envVarName: String, container: Container) : Container = {
+    val builder = new ContainerBuilder(container)
+    if (conf.get(confKey, "").isEmpty) {
+      builder.addNewEnv()
+        .withName(envVarName)
+        .withValueFrom(new EnvVarSourceBuilder()
+          .withNewFieldRef("v1", "status.podIP")
+          .build())
+        .endEnv()
+    } else {
+      builder.addNewEnv().withName(envVarName)
+        .withValue(conf.get(confKey, ALL_IPS))
+        .endEnv()
+    }
+    builder.build()
+  }
+
+  def getContainerWithPrePostScriptsEnv(conf: KubernetesConf,
+                                        container: Container) : Container = {
+    new ContainerBuilder(container)
+      .addNewEnv()
+      .withName(ENV_PRE_START_SCRIPT)
+      .withValue(conf.get(KUBERNETES_PRE_START_SCRIPT.key, ""))
+      .endEnv()
+      .addNewEnv()
+      .withName(ENV_POST_STOP_SCRIPT)
+      .withValue(conf.get(KUBERNETES_POST_STOP_SCRIPT.key, ""))
+      .endEnv()
+      .build()
+  }
+
   @Since("3.0.0")
   def requireBothOrNeitherDefined(
       opt1: Option[_],
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala
index 2b287ea856046..dbe91d71e34b5 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala
@@ -27,6 +27,7 @@ import org.apache.spark.SparkException
 import org.apache.spark.deploy.k8s._
 import org.apache.spark.deploy.k8s.Config._
 import org.apache.spark.deploy.k8s.Constants._
+import org.apache.spark.deploy.k8s.KubernetesUtils._
 import org.apache.spark.deploy.k8s.submit._
 import org.apache.spark.internal.config._
 import org.apache.spark.resource.ResourceProfile
@@ -83,6 +84,7 @@ private[spark] class BasicDriverFeatureStep(conf: KubernetesDriverConf)
   private val driverMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB

   override def configurePod(pod: SparkPod): SparkPod = {
+    verifyBindAddress(conf, DRIVER_BIND_ADDRESS.key)
     val driverCustomEnvs = KubernetesUtils.buildEnvVars(
       Seq(ENV_APPLICATION_ID -> conf.appId) ++ conf.environment)
     val driverCpuQuantity = new Quantity(driverCoresRequest)
@@ -124,12 +126,6 @@ private[spark] class BasicDriverFeatureStep(conf: KubernetesDriverConf)
         .withValue(Utils.getCurrentUserName())
         .endEnv()
       .addAllToEnv(driverCustomEnvs.asJava)
-      .addNewEnv()
-        .withName(ENV_DRIVER_BIND_ADDRESS)
-        .withValueFrom(new EnvVarSourceBuilder()
-          .withNewFieldRef("v1", "status.podIP")
-          .build())
-        .endEnv()
       .editOrNewResources()
         .addToRequests("cpu", driverCpuQuantity)
         .addToLimits(maybeCpuLimitQuantity.toMap.asJava)
@@ -139,6 +135,12 @@ private[spark] class BasicDriverFeatureStep(conf: KubernetesDriverConf)
         .endResources()
       .build()

+    val containerWithBindAddress = getContainerWithBindAddressEnv(conf,
+      DRIVER_BIND_ADDRESS.key, ENV_DRIVER_BIND_ADDRESS, driverContainer)
+
+    val containerWithPrePostScriptsEnv =
+      getContainerWithPrePostScriptsEnv(conf, containerWithBindAddress)
+
     val driverPod = new PodBuilder(pod.pod)
       .editOrNewMetadata()
         .withName(driverPodName)
@@ -157,7 +159,7 @@ private[spark] class BasicDriverFeatureStep(conf: KubernetesDriverConf)
     conf.schedulerName
       .foreach(driverPod.getSpec.setSchedulerName)

-    SparkPod(driverPod, driverContainer)
+    SparkPod(driverPod, containerWithPrePostScriptsEnv)
   }

   override def getAdditionalPodSystemProperties(): Map[String, String] = {
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala
index 0b0bbc30ba41a..fe73201b49aee 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala
@@ -24,6 +24,7 @@ import org.apache.spark.{SecurityManager, SparkConf, SparkException}
 import org.apache.spark.deploy.k8s._
 import org.apache.spark.deploy.k8s.Config._
 import org.apache.spark.deploy.k8s.Constants._
+import org.apache.spark.deploy.k8s.KubernetesUtils._
 import org.apache.spark.deploy.k8s.submit.KubernetesClientUtils
 import org.apache.spark.internal.Logging
 import org.apache.spark.internal.config._
@@ -100,6 +101,7 @@ private[spark] class BasicExecutorFeatureStep(
   }

   override def configurePod(pod: SparkPod): SparkPod = {
+    verifyBindAddress(kubernetesConf, EXECUTOR_BIND_ADDRESS.key)
     val name = s"$executorPodNamePrefix-exec-${kubernetesConf.executorId}"
     val configMapName = KubernetesClientUtils.configMapNameExecutor
     val confFilesMap = KubernetesClientUtils
@@ -241,6 +243,11 @@ private[spark] class BasicExecutorFeatureStep(
           .endLifecycle()
           .build()
       }
+    val containerWithBindAddress = getContainerWithBindAddressEnv(
+      kubernetesConf, EXECUTOR_BIND_ADDRESS.key, ENV_EXECUTOR_BIND_ADDRESS, containerWithLifecycle)
+
+    val containerWithPrePostScripts =
+      getContainerWithPrePostScriptsEnv(kubernetesConf, containerWithBindAddress)
     val ownerReference = kubernetesConf.driverPod.map { pod =>
       new OwnerReferenceBuilder()
         .withController(true)
@@ -288,6 +295,6 @@ private[spark] class BasicExecutorFeatureStep(
     kubernetesConf.schedulerName
       .foreach(executorPod.getSpec.setSchedulerName)

-    SparkPod(executorPod, containerWithLifecycle)
+    SparkPod(executorPod, containerWithPrePostScripts)
   }
 }
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala
index 37dfe8ec07a4c..cfc11cd2096aa 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala
@@ -23,6 +23,7 @@ import io.fabric8.kubernetes.api.model.{HasMetadata, ServiceBuilder}
 import org.apache.spark.deploy.k8s.{KubernetesDriverConf, KubernetesUtils, SparkPod}
 import org.apache.spark.deploy.k8s.Config.{KUBERNETES_DNS_LABEL_NAME_MAX_LENGTH, KUBERNETES_DRIVER_SERVICE_IP_FAMILIES, KUBERNETES_DRIVER_SERVICE_IP_FAMILY_POLICY}
 import org.apache.spark.deploy.k8s.Constants._
+import org.apache.spark.deploy.k8s.KubernetesUtils._
 import org.apache.spark.internal.{config, Logging}
 import org.apache.spark.util.{Clock, SystemClock}

@@ -32,9 +33,7 @@ private[spark] class DriverServiceFeatureStep(
   extends KubernetesFeatureConfigStep with Logging {
   import DriverServiceFeatureStep._

-  require(kubernetesConf.getOption(DRIVER_BIND_ADDRESS_KEY).isEmpty,
-    s"$DRIVER_BIND_ADDRESS_KEY is not supported in Kubernetes mode, as the driver's bind " +
-      "address is managed and set to the driver pod's IP address.")
+  verifyBindAddress(kubernetesConf, DRIVER_BIND_ADDRESS_KEY)
   require(kubernetesConf.getOption(DRIVER_HOST_KEY).isEmpty,
     s"$DRIVER_HOST_KEY is not supported in Kubernetes mode, as the driver's hostname will be " +
       "managed via a Kubernetes service.")
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/ExecutorServiceFeatureStep.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/ExecutorServiceFeatureStep.scala
new file mode 100644
index 0000000000000..d9e037a7a0b37
--- /dev/null
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/ExecutorServiceFeatureStep.scala
@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.deploy.k8s.features
+
+import scala.collection.JavaConverters._
+
+import io.fabric8.kubernetes.api.model.{HasMetadata, ServiceBuilder}
+
+import org.apache.spark.deploy.k8s.{KubernetesExecutorConf, KubernetesUtils, SparkPod}
+import org.apache.spark.deploy.k8s.Constants._
+import org.apache.spark.internal.{config, Logging}
+import org.apache.spark.util.{Clock, SystemClock}
+
+private[spark] class ExecutorServiceFeatureStep(
+                                                 kubernetesConf: KubernetesExecutorConf,
+                                                 clock: Clock = new SystemClock())
+  extends KubernetesFeatureConfigStep with Logging {
+  import ExecutorServiceFeatureStep._
+
+  private val preferredServiceName = s"${kubernetesConf.resourceNamePrefix}" +
+    s"-${kubernetesConf.executorId}$EXECUTOR_SVC_POSTFIX"
+  private val resolvedServiceName = if (preferredServiceName.length <= MAX_SERVICE_NAME_LENGTH) {
+    preferredServiceName
+  } else {
+    val randomServiceId = KubernetesUtils.uniqueID(clock = clock)
+    val shorterServiceName = s"spark-$randomServiceId$EXECUTOR_SVC_POSTFIX"
+    logWarning(s"Executor's hostname would preferably be $preferredServiceName, but this is " +
+      s"too long (must be <= $MAX_SERVICE_NAME_LENGTH characters). Falling back to use " +
+      s"$shorterServiceName as the executor service's name.")
+    shorterServiceName
+  }
+
+  private val executorBlockManagerPort = kubernetesConf.sparkConf.getInt(
+    config.BLOCK_MANAGER_PORT.key, DEFAULT_BLOCKMANAGER_PORT)
+
+
+  override def configurePod(pod: SparkPod): SparkPod = {
+    val executorHostName = s"$resolvedServiceName.${kubernetesConf.namespace}.svc"
+    logInfo(s"Adding service step $executorHostName")
+    pod.container.getEnv.forEach { x =>
+      if (x.getName == ENV_EXECUTOR_POD_IP) {
+        x.setValue(executorHostName)
+        x.setValueFrom(null)
+      }
+    }
+    pod
+  }
+
+  override def getAdditionalKubernetesResources(): Seq[HasMetadata] = {
+    val executorService = new ServiceBuilder()
+      .withNewMetadata()
+      .withName(resolvedServiceName)
+      .addToAnnotations(kubernetesConf.serviceAnnotations.asJava)
+      .endMetadata()
+      .withNewSpec()
+      .withClusterIP("None")
+      .withSelector(kubernetesConf.labels.asJava)
+      .addNewPort()
+      .withName(BLOCK_MANAGER_PORT_NAME)
+      .withPort(executorBlockManagerPort)
+      .withNewTargetPort(executorBlockManagerPort)
+      .endPort()
+      .endSpec()
+      .build()
+    Seq(executorService)
+  }
+}
+
+private[spark] object ExecutorServiceFeatureStep {
+  val EXECUTOR_SVC_POSTFIX = "-executor-svc"
+  val MAX_SERVICE_NAME_LENGTH = 63
+}
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala
index 7174ba95927e7..f2ec4e647e49b 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala
@@ -441,16 +441,20 @@ class ExecutorPodsAllocator(
       try {
         addOwnerReference(createdExecutorPod, resources)
         resources
-          .filter(_.getKind == "PersistentVolumeClaim")
           .foreach { resource =>
-            if (conf.get(KUBERNETES_DRIVER_OWN_PVC) && driverPod.nonEmpty) {
-              addOwnerReference(driverPod.get, Seq(resource))
+            if (resource.getKind == "PersistentVolumeClaim") {
+              if (conf.get(KUBERNETES_DRIVER_OWN_PVC) && driverPod.nonEmpty) {
+                addOwnerReference(driverPod.get, Seq(resource))
+              }
+              val pvc = resource.asInstanceOf[PersistentVolumeClaim]
+              logInfo(s"Trying to create PersistentVolumeClaim ${pvc.getMetadata.getName} " +
+                s"with StorageClass ${pvc.getSpec.getStorageClassName}")
+              kubernetesClient.persistentVolumeClaims().inNamespace(namespace).
+                resource(pvc).create()
+              PVC_COUNTER.incrementAndGet()
+            } else {
+              kubernetesClient.resource(resource).inNamespace(namespace).create()
             }
-            val pvc = resource.asInstanceOf[PersistentVolumeClaim]
-            logInfo(s"Trying to create PersistentVolumeClaim ${pvc.getMetadata.getName} with " +
-              s"StorageClass ${pvc.getSpec.getStorageClassName}")
-            kubernetesClient.persistentVolumeClaims().inNamespace(namespace).resource(pvc).create()
-            PVC_COUNTER.incrementAndGet()
           }
         newlyCreatedExecutors(newExecutorId) = (resourceProfileId, clock.getTimeMillis())
         logDebug(s"Requested executor with id $newExecutorId from Kubernetes.")
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilder.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilder.scala
index 67aad00f98543..328be41211a9a 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilder.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilder.scala
@@ -65,8 +65,9 @@ private[spark] class KubernetesExecutorBuilder {
         }
       }

-    val features = Seq(
-      new BasicExecutorFeatureStep(conf, secMgr, resourceProfile),
+    val features =
+      Seq(new BasicExecutorFeatureStep(conf, secMgr, resourceProfile)
+      ) ++ serviceFeatureStep(conf) ++ Seq (
       new ExecutorKubernetesCredentialsFeatureStep(conf),
       new MountSecretsFeatureStep(conf),
       new EnvSecretsFeatureStep(conf),
@@ -88,4 +89,9 @@ private[spark] class KubernetesExecutorBuilder {
     }
   }

+  private def serviceFeatureStep(conf: KubernetesExecutorConf) = {
+    if (conf.get(Config.KUBERNETES_EXECUTORS_SVC)) {
+      Seq(new ExecutorServiceFeatureStep(conf))
+    } else Seq()
+  }
 }
diff --git a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesTestConf.scala b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesTestConf.scala
index d6a60b1edea2f..ca91929a144d7 100644
--- a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesTestConf.scala
+++ b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesTestConf.scala
@@ -74,17 +74,22 @@ object KubernetesTestConf {
   def createExecutorConf(
       sparkConf: SparkConf = DEFAULT_CONF,
       driverPod: Option[Pod] = None,
+      resourceNamePrefix: Option[String] = None,
       labels: Map[String, String] = Map.empty,
       environment: Map[String, String] = Map.empty,
       annotations: Map[String, String] = Map.empty,
+      serviceAnnotations: Map[String, String] = Map.empty,
       secretEnvNamesToKeyRefs: Map[String, String] = Map.empty,
       secretNamesToMountPaths: Map[String, String] = Map.empty,
       volumes: Seq[KubernetesVolumeSpec] = Seq.empty): KubernetesExecutorConf = {
     val conf = sparkConf.clone()
-
+    resourceNamePrefix.foreach { prefix =>
+      conf.set(KUBERNETES_EXECUTOR_POD_NAME_PREFIX, prefix)
+    }
     setPrefixedConfigs(conf, KUBERNETES_EXECUTOR_LABEL_PREFIX, labels)
     setPrefixedConfigs(conf, "spark.executorEnv.", environment)
     setPrefixedConfigs(conf, KUBERNETES_EXECUTOR_ANNOTATION_PREFIX, annotations)
+    setPrefixedConfigs(conf, KUBERNETES_EXECUTOR_SERVICE_ANNOTATION_PREFIX, serviceAnnotations)
     setPrefixedConfigs(conf, KUBERNETES_EXECUTOR_SECRETS_PREFIX, secretNamesToMountPaths)
     setPrefixedConfigs(conf, KUBERNETES_EXECUTOR_SECRET_KEY_REF_PREFIX, secretEnvNamesToKeyRefs)
     setVolumeSpecs(conf, KUBERNETES_EXECUTOR_VOLUMES_PREFIX, volumes)
diff --git a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStepSuite.scala b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStepSuite.scala
index 9eb27a37fbabc..ad10822da387e 100644
--- a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStepSuite.scala
+++ b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStepSuite.scala
@@ -171,6 +171,34 @@ class BasicDriverFeatureStepSuite extends SparkFunSuite {
     }
   }

+  test("test invalid bind address") {
+    val baseConf = KubernetesTestConf.createDriverConf()
+    baseConf.sparkConf.set(DRIVER_BIND_ADDRESS, "host")
+    baseConf.sparkConf.set(DRIVER_CONTAINER_IMAGE.key, "test")
+
+    val error = intercept[IllegalArgumentException] {
+      val step = new BasicDriverFeatureStep(baseConf)
+      val driver = step.configurePod(SparkPod.initialPod())
+    }.getMessage()
+    assert(error.contains(s"${DRIVER_BIND_ADDRESS.key} is not supported in Kubernetes mode," +
+      s" as the bind address can either be $ALL_IPS" +
+      s" or the pod's IP address."))
+  }
+
+  test("test binding to all IPs") {
+    val baseConf = KubernetesTestConf.createDriverConf()
+    baseConf.sparkConf.set(DRIVER_BIND_ADDRESS, ALL_IPS)
+    baseConf.sparkConf.set(DRIVER_CONTAINER_IMAGE.key, "test")
+    val step = new BasicDriverFeatureStep(baseConf)
+    val driver = step.configurePod(SparkPod.initialPod())
+    assert(driver.container.getEnv.asScala
+      .filter(env => env.getName == ENV_DRIVER_BIND_ADDRESS).size == 1)
+    driver.container.getEnv.forEach(env =>
+      if (env.getName() == ENV_DRIVER_BIND_ADDRESS ) {
+        assert(env.getValue == ALL_IPS)
+      } )
+  }
+
   test("Check appropriate entrypoint rerouting for various bindings") {
     val javaSparkConf = new SparkConf()
       .set(DRIVER_MEMORY.key, "4g")
diff --git a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala
index 32897014931cf..b01b2ef5dc855 100644
--- a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala
+++ b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala
@@ -120,6 +120,33 @@ class BasicExecutorFeatureStepSuite extends SparkFunSuite with BeforeAndAfter {
     assert(error.contains("You must specify an amount for gpu"))
   }

+  test("test invalid bind address") {
+    baseConf.set(EXECUTOR_BIND_ADDRESS, "host")
+    val error = intercept[IllegalArgumentException] {
+      initDefaultProfile(baseConf)
+      val step = new BasicExecutorFeatureStep(newExecutorConf(), new SecurityManager(baseConf),
+        defaultProfile)
+      val executor = step.configurePod(SparkPod.initialPod())
+    }.getMessage()
+    assert(error.contains(s"${EXECUTOR_BIND_ADDRESS.key} is not supported in Kubernetes mode," +
+      s" as the bind address can either be $ALL_IPS" +
+      s" or the pod's IP address."))
+  }
+
+  test("test binding to all IPs") {
+    baseConf.set(EXECUTOR_BIND_ADDRESS, ALL_IPS)
+    initDefaultProfile(baseConf)
+    val step = new BasicExecutorFeatureStep(newExecutorConf(), new SecurityManager(baseConf),
+      defaultProfile)
+    val executor = step.configurePod(SparkPod.initialPod())
+    assert(executor.container.getEnv.asScala
+      .filter(env => env.getName == ENV_EXECUTOR_BIND_ADDRESS).size == 1)
+    executor.container.getEnv.forEach(env =>
+      if (env.getName() == ENV_EXECUTOR_BIND_ADDRESS ) {
+        assert(env.getValue == ALL_IPS)
+      } )
+  }
+
   test("basic executor pod with resources") {
     val fpgaResourceID = new ResourceID(SPARK_EXECUTOR_PREFIX, FPGA)
     val gpuExecutorResourceID = new ResourceID(SPARK_EXECUTOR_PREFIX, GPU)
@@ -547,6 +574,9 @@ class BasicExecutorFeatureStepSuite extends SparkFunSuite with BeforeAndAfter {
       ENV_RESOURCE_PROFILE_ID -> "0",
       // These are populated by K8s on scheduling
       ENV_EXECUTOR_POD_IP -> null,
+      ENV_EXECUTOR_BIND_ADDRESS -> null,
+      ENV_PRE_START_SCRIPT -> "",
+      ENV_POST_STOP_SCRIPT -> "",
       ENV_EXECUTOR_POD_NAME -> null)

     val extraJavaOptsStart = additionalEnvVars.keys.count(_.startsWith(ENV_JAVA_OPT_PREFIX))
diff --git a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStepSuite.scala b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStepSuite.scala
index 609c80f27c3da..ecc426bc82171 100644
--- a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStepSuite.scala
+++ b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStepSuite.scala
@@ -149,9 +149,9 @@ class DriverServiceFeatureStepSuite extends SparkFunSuite {
       new DriverServiceFeatureStep(KubernetesTestConf.createDriverConf(sparkConf = sparkConf))
     }
     assert(e1.getMessage ===
-      s"requirement failed: ${DriverServiceFeatureStep.DRIVER_BIND_ADDRESS_KEY} is" +
-      " not supported in Kubernetes mode, as the driver's bind address is managed" +
-      " and set to the driver pod's IP address.")
+      s"requirement failed: ${DriverServiceFeatureStep.DRIVER_BIND_ADDRESS_KEY} is " +
+      s"not supported in Kubernetes mode, as the bind address can either be $ALL_IPS" +
+      s" or the pod's IP address.")

     sparkConf.remove(DRIVER_BIND_ADDRESS)
     sparkConf.set(DRIVER_HOST_ADDRESS, "host")
diff --git a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/ExecutorServiceFeatureStepSuite.scala b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/ExecutorServiceFeatureStepSuite.scala
new file mode 100644
index 0000000000000..63a06d145f72b
--- /dev/null
+++ b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/ExecutorServiceFeatureStepSuite.scala
@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.deploy.k8s.features
+
+import io.fabric8.kubernetes.api.model.{EnvVarBuilder, Service}
+import scala.collection.JavaConverters._
+
+import org.apache.spark.{SparkConf, SparkFunSuite}
+import org.apache.spark.deploy.k8s.{KubernetesExecutorConf, KubernetesTestConf, SparkPod}
+import org.apache.spark.deploy.k8s.Config._
+import org.apache.spark.deploy.k8s.Constants._
+import org.apache.spark.internal.config._
+import org.apache.spark.util.ManualClock
+
+
+class ExecutorServiceFeatureStepSuite extends SparkFunSuite {
+
+  private val LONG_RESOURCE_NAME_PREFIX =
+    "a" * (ExecutorServiceFeatureStep.MAX_SERVICE_NAME_LENGTH -
+      ExecutorServiceFeatureStep.EXECUTOR_SVC_POSTFIX.length + 1)
+  private val EXECUTOR_LABELS = Map(
+    "label1key" -> "label1value",
+    "label2key" -> "label2value")
+  private val EXECUTOR_SERVICE_ANNOTATIONS = Map(
+    "annotation1key" -> "annotation1value",
+    "annotation2key" -> "annotation2value")
+
+  test("Headless service correct port for block manager") {
+    val sparkConf = new SparkConf(false)
+      .set(BLOCK_MANAGER_PORT, 9000)
+    val kconf = KubernetesTestConf.createExecutorConf(
+      sparkConf = sparkConf,
+      labels = EXECUTOR_LABELS,
+      serviceAnnotations = EXECUTOR_SERVICE_ANNOTATIONS)
+    val configurationStep = new ExecutorServiceFeatureStep(kconf)
+    assert(configurationStep.configurePod(SparkPod.initialPod()) === SparkPod.initialPod())
+    assert(configurationStep.getAdditionalKubernetesResources().size === 1)
+    assert(configurationStep.getAdditionalKubernetesResources().head.isInstanceOf[Service])
+    val executorService = configurationStep
+      .getAdditionalKubernetesResources()
+      .head
+      .asInstanceOf[Service]
+    verifyService(
+      9000,
+      expectedServiceName(kconf),
+      executorService)
+  }
+
+  test("Confirm pod IP is correctly post processed") {
+    val sparkConf = new SparkConf(false)
+      .set(BLOCK_MANAGER_PORT, 9000)
+      .set(KUBERNETES_NAMESPACE, "my-namespace")
+    val kconf = KubernetesTestConf.createExecutorConf(
+      sparkConf = sparkConf,
+      labels = EXECUTOR_LABELS)
+    val configurationStep = new ExecutorServiceFeatureStep(kconf)
+    val serviceName = expectedServiceName(kconf)
+    val expectedHostName = s"$serviceName.my-namespace.svc"
+    val emptyPod = SparkPod.initialPod()
+    emptyPod.container.getEnv.add(new EnvVarBuilder().
+      withName(ENV_EXECUTOR_POD_IP).withValue("dummy").build())
+    val postConfiguredPod = configurationStep.configurePod(emptyPod)
+    assert(postConfiguredPod.container.getEnv.asScala.filter
+    (envVar => envVar.getName== ENV_EXECUTOR_POD_IP).head.getValue == expectedHostName)
+  }
+
+  private def expectedServiceName(kconf: KubernetesExecutorConf) = {
+    s"${kconf.resourceNamePrefix}-${kconf.executorId}" +
+      s"${ExecutorServiceFeatureStep.EXECUTOR_SVC_POSTFIX}"
+  }
+
+  test("Ports should resolve to defaults in SparkConf and in the service.") {
+    val kconf = KubernetesTestConf.createExecutorConf(
+      labels = EXECUTOR_LABELS,
+      serviceAnnotations = EXECUTOR_SERVICE_ANNOTATIONS)
+    val configurationStep = new ExecutorServiceFeatureStep(kconf)
+    val resolvedService = configurationStep
+      .getAdditionalKubernetesResources()
+      .head
+      .asInstanceOf[Service]
+    verifyService(
+      DEFAULT_BLOCKMANAGER_PORT,
+      expectedServiceName(kconf),
+      resolvedService)
+  }
+
+  test("Long prefixes should switch to using a generated unique name.") {
+    val sparkConf = new SparkConf(false)
+      .set(KUBERNETES_NAMESPACE, "my-namespace")
+    val kconf = KubernetesTestConf.createExecutorConf(
+      sparkConf = sparkConf,
+      resourceNamePrefix = Some(LONG_RESOURCE_NAME_PREFIX),
+      labels = EXECUTOR_LABELS)
+    val clock = new ManualClock()
+
+    // Ensure that multiple services created at the same time generate unique names.
+    val services = (1 to 10).map { _ =>
+      val configurationStep = new ExecutorServiceFeatureStep(kconf, clock = clock)
+      val serviceName = configurationStep
+        .getAdditionalKubernetesResources()
+        .head
+        .asInstanceOf[Service]
+        .getMetadata
+        .getName
+
+      serviceName
+    }.toSet
+
+    assert(services.size === 10)
+    services.foreach { case (name) =>
+      assert(!name.startsWith(kconf.resourceNamePrefix))
+    }
+  }
+
+  private def verifyService(
+                             blockManagerPort: Int,
+                             expectedServiceName: String,
+                             service: Service): Unit = {
+    assert(service.getMetadata.getName === expectedServiceName)
+    assert(service.getSpec.getClusterIP === "None")
+    EXECUTOR_LABELS.foreach { case (k, v) =>
+      assert(service.getSpec.getSelector.get(k) === v)
+    }
+    EXECUTOR_SERVICE_ANNOTATIONS.foreach { case (k, v) =>
+      assert(service.getMetadata.getAnnotations.get(k) === v)
+    }
+    assert(service.getSpec.getPorts.size() === 1)
+    val driverServicePorts = service.getSpec.getPorts.asScala
+    assert(driverServicePorts.head.getName === BLOCK_MANAGER_PORT_NAME)
+    assert(driverServicePorts.head.getPort.intValue() === blockManagerPort)
+    assert(driverServicePorts.head.getTargetPort.getIntVal === blockManagerPort)
+  }
+}
diff --git a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilderSuite.scala b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilderSuite.scala
index 17c2d4a938c14..53e20ce778432 100644
--- a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilderSuite.scala
+++ b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilderSuite.scala
@@ -17,6 +17,7 @@
 package org.apache.spark.scheduler.cluster.k8s

 import io.fabric8.kubernetes.client.KubernetesClient
+import org.mockito.Mockito.mock

 import org.apache.spark.{SecurityManager, SparkConf}
 import org.apache.spark.deploy.k8s._
@@ -61,6 +62,28 @@ class KubernetesExecutorBuilderSuite extends PodBuilderSuite {
     val defaultProfile = ResourceProfile.getOrCreateDefaultProfile(sparkConf)
     new KubernetesExecutorBuilder().buildFromFeatures(conf, secMgr, client, defaultProfile).pod
   }
+
+  test("Verify service resource is not created with property default") {
+    val conf = KubernetesTestConf.createExecutorConf()
+    verifyExecutorSvcCreation(conf, 0)
+  }
+
+  private def verifyExecutorSvcCreation(conf: KubernetesExecutorConf, numOfSvc: Int) = {
+    conf.sparkConf.set(Config.CONTAINER_IMAGE.key, "test-image")
+    val secMgr = new SecurityManager(conf.sparkConf)
+    val defaultProfile = ResourceProfile.getOrCreateDefaultProfile(conf.sparkConf)
+    val client = mock(classOf[KubernetesClient])
+    val execSpec = new KubernetesExecutorBuilder().
+      buildFromFeatures(conf, secMgr, client, defaultProfile)
+    assert(execSpec.executorKubernetesResources.filter(
+      resource => resource.getKind == "Service").size == numOfSvc)
+  }
+
+  test("Verify service resource is created with property enabled") {
+    val conf = KubernetesTestConf.createExecutorConf()
+    conf.sparkConf.set(Config.KUBERNETES_EXECUTORS_SVC.key, "true")
+    verifyExecutorSvcCreation(conf, 1)
+  }
 }

 /**
diff --git a/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh b/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh
index 42f4df88f3da9..24c908d03a4be 100755
--- a/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh
+++ b/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh
@@ -95,6 +95,7 @@ case "$1" in
       -Xmx$SPARK_EXECUTOR_MEMORY
       -cp "$SPARK_CLASSPATH:$SPARK_DIST_CLASSPATH"
       org.apache.spark.scheduler.cluster.k8s.KubernetesExecutorBackend
+      --bind-address $SPARK_EXECUTOR_BIND_ADDRESS
       --driver-url $SPARK_DRIVER_URL
       --executor-id $SPARK_EXECUTOR_ID
       --cores $SPARK_EXECUTOR_CORES
@@ -111,5 +112,15 @@ case "$1" in
     ;;
 esac

+CMD_STRING="${CMD[@]}"
+
+if ! [ -z "${SPARK_PRE_START_SCRIPT}" ]; then
+  CMD_STRING="$SPARK_PRE_START_SCRIPT && $CMD_STRING";
+fi
+
+if ! [ -z "${SPARK_POST_STOP_SCRIPT}" ]; then
+  CMD_STRING="$CMD_STRING ; $SPARK_POST_STOP_SCRIPT";
+fi
+
 # Execute the container CMD under tini for better hygiene
-exec /usr/bin/tini -s -- "${CMD[@]}"
+exec /usr/bin/tini -sg -- /bin/bash -c "$CMD_STRING"
\ No newline at end of file
